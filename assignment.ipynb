{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qjy9ey/linear_models_assignment/blob/main/assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c7b14a-e5aa-4abc-b48b-b8a9d20dacac",
      "metadata": {
        "id": "e0c7b14a-e5aa-4abc-b48b-b8a9d20dacac"
      },
      "source": [
        "# Assignment: Linear Models\n",
        "## Do two questions in total: \"Q1+Q2\" or \"Q1+Q3\"\n",
        "### `! git clone https://github.com/ds3001f25/linear_models_assignment.git`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca1cfba3",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ca1cfba3"
      },
      "source": [
        "**Q1.** Let's explore multiple linear regression in a two-variable case, to build more intuition about what is happening.\n",
        "\n",
        "Suppose the model is\n",
        "$$\n",
        "\\hat{y}_i = b_0 + b_1 z_{i1} + b_2 z_{i2}\n",
        "$$\n",
        "Assume that $z_{ij}$ is centered or de-meaned, so that $z_{ij} = x_{ij} - m_j$ where $m_j$ is the mean of variable $j$ and $x_{ij}$ is the original value of variable $j$ for observation $i$. Notice that this implies\n",
        "$$\n",
        "\\dfrac{1}{N} \\sum_{i=1}^N z_{ij} = 0\n",
        "$$\n",
        "which will simplify your calculations below substantially!\n",
        "\n",
        "1. Write down the SSE for this model.\n",
        "2. Take partial derivatives with respect to $b_0$, $b_1$, and $b_2$.\n",
        "3. Verify that the average error is zero and $e \\cdot z =0$ at the optimum, just as in the single linear regression case.\n",
        "4. Show that the optimal intercept is $b_0^* = \\bar{y}$. Eliminate $b_0^*$ from the remaining equations, and focus on $b_1$ and $b_2$.\n",
        "5. Write your results as a matrix equation in the form \"$Ab=C$\". These are called the **normal equations**.\n",
        "6. Divide both sides by $N$ and substitute $z_{ij} = x_{ij} - m_j$ back into your normal equations for $x_{ij}$. What is the matrix $A$? What is the vector $C$? Explain the intuition of your discovery."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   \n",
        "The residuals are:\n",
        "\n",
        "$$\n",
        "e^i = y^i - \\hat{y}^i = y^i - (b_0 + b_1 z_{i1} + b_2 z_{i2})\n",
        "$$\n",
        "\n",
        "The SSE is:\n",
        "\n",
        "$$\n",
        "SSE = \\sum_{i=1}^N (e^i)^2 = \\sum_{i=1}^N \\big(y^i - b_0 - b_1 z_{i1} - b_2 z_{i2}\\big)^2\n",
        "$$\n",
        "\n",
        "\n",
        "---\n",
        "2.   \n",
        "Take derivatives of SSE with respect to the parameters and set\n",
        "\n",
        "$$\n",
        "\\frac{\\partial SSE}{\\partial b_0} = -2 \\sum_{i=1}^N (y^i - b_0 - b_1 z_{i1} - b_2 z_{i2}) = 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial SSE}{\\partial b_1} = -2 \\sum_{i=1}^N z_{i1} (y^i - b_0 - b_1 z_{i1} - b_2 z_{i2}) = 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial SSE}{\\partial b_2} = -2 \\sum_{i=1}^N z_{i2} (y^i - b_0 - b_1 z_{i1} - b_2 z_{i2}) = 0\n",
        "$$\n",
        "\n",
        "---\n",
        "3.   \n",
        "\n",
        "From the derivatives:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^N e^i = 0 \\quad \\Rightarrow \\quad \\text{mean residual } \\bar{e} = 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^N e^i z_{i1} = 0, \\quad \\sum_{i=1}^N e^i z_{i2} = 0\n",
        "$$\n",
        "\n",
        "Residuals are orthogonal to each predictor, just like in simple linear regression.\n",
        "\n",
        "---\n",
        "\n",
        "4.   \n",
        "\n",
        "Since the predictors are centered, we have:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^N z_{i1} = \\sum_{i=1}^N z_{i2} = 0\n",
        "$$\n",
        "\n",
        "Then, from the first normal equation:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^N y^i - N b_0 = 0\n",
        "\\quad \\Rightarrow \\quad\n",
        "b_0^* = \\bar{y}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "5.   \n",
        "\n",
        "Define $\\tilde{y}_i = y_i - \\bar{y}$.  \n",
        "Then the remaining equations are:\n",
        "\n",
        "$$\n",
        "\\sum z_{i1} (\\tilde{y}_i - b_1 z_{i1} - b_2 z_{i2}) = 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sum z_{i2} (\\tilde{y}_i - b_1 z_{i1} - b_2 z_{i2}) = 0\n",
        "$$\n",
        "\n",
        "Expanding gives the **matrix form**:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\sum z_{i1}^2 & \\sum z_{i1} z_{i2} \\\\\n",
        "\\sum z_{i1} z_{i2} & \\sum z_{i2}^2\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\sum z_{i1} \\tilde{y}_i \\\\\n",
        "\\sum z_{i2} \\tilde{y}_i\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This is the **normal equation** $A b = C$.\n",
        "\n",
        "---\n",
        "\n",
        "6.   \n",
        "\n",
        "Divide sums by $N$ and replace $z_{ij} = x_{ij} - m_j$:\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "\\mathrm{Var}(x_1) & \\mathrm{Cov}(x_1, x_2) \\\\\n",
        "\\mathrm{Cov}(x_1, x_2) & \\mathrm{Var}(x_2)\n",
        "\\end{bmatrix},\n",
        "\\quad\n",
        "C =\n",
        "\\begin{bmatrix}\n",
        "\\mathrm{Cov}(x_1, y) \\\\\n",
        "\\mathrm{Cov}(x_2, y)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The slopes depend on the variances and covariances of the predictors. The intercept is simply the mean of $y$. Residuals remain orthogonal to all regressors, ensuring the best linear unbiased estimates."
      ],
      "metadata": {
        "id": "jq4pi2RqwqbK"
      },
      "id": "jq4pi2RqwqbK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b005058d",
      "metadata": {
        "id": "b005058d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "95f22300-0180-4ed2-be8f-ed56cf4cd36b",
      "metadata": {
        "id": "95f22300-0180-4ed2-be8f-ed56cf4cd36b"
      },
      "source": [
        "**Q2.** This question is a case study for linear models. The data are about car prices. In particular, they include:\n",
        "\n",
        "  - `Price`, `Color`, `Seating_Capacity`\n",
        "  - `Body_Type`: crossover, hatchback, muv, sedan, suv\n",
        "  - `Make`, `Make_Year`: The brand of car and year produced\n",
        "  - `Mileage_Run`: The number of miles on the odometer\n",
        "  - `Fuel_Type`: Diesel or gasoline/petrol\n",
        "  - `Transmission`, `Transmission_Type`:  speeds and automatic/manual\n",
        "\n",
        "  1. Load `cars_hw.csv`. These data were really dirty, and I've already cleaned them a significant amount in terms of missing values and other issues, but some issues remain (e.g. outliers, badly scaled variables that require a log or arcsinh transformation). Clean the data however you think is most appropriate.\n",
        "  2. Summarize the `Price` variable and create a kernel density plot. Use `.groupby()` and `.describe()` to summarize prices by brand (`Make`). Make a grouped kernel density plot by `Make`. Which car brands are the most expensive? What do prices look like in general?\n",
        "  3. Split the data into an 80% training set and a 20% testing set.\n",
        "  4. Make a model where you regress price on the numeric variables alone; what is the $R^2$ and `RMSE` on the training set and test set? Make a second model where, for the categorical variables, you regress price on a model comprised of one-hot encoded regressors/features alone (you can use `pd.get_dummies()`; be careful of the dummy variable trap); what is the $R^2$ and `RMSE` on the test set? Which model performs better on the test set? Make a third model that combines all the regressors from the previous two; what is the $R^2$ and `RMSE` on the test set? Does the joint model perform better or worse, and by home much?\n",
        "  5. Use the `PolynomialFeatures` function from `sklearn` to expand the set of numerical variables you're using in the regression. As you increase the degree of the expansion, how do the $R^2$ and `RMSE` change? At what point does $R^2$ go negative on the test set? For your best model with expanded features, what is the $R^2$ and `RMSE`? How does it compare to your best model from part 4?\n",
        "  6. For your best model so far, determine the predicted values for the test data and plot them against the true values. Do the predicted values and true values roughly line up along the diagonal, or not? Compute the residuals/errors for the test data and create a kernel density plot. Do the residuals look roughly bell-shaped around zero? Evaluate the strengths and weaknesses of your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0acc2b86",
      "metadata": {
        "id": "0acc2b86"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "aedcd486",
      "metadata": {
        "id": "aedcd486"
      },
      "source": [
        "**Q3.** This question refers to the `heart_hw.csv` data. It contains three variables:\n",
        "\n",
        "  - `y`: Whether the individual survived for three years, coded 0 for death and 1 for survival\n",
        "  - `age`: Patient's age\n",
        "  - `transplant`: `control` for not receiving a transplant and `treatment` for receiving a transplant\n",
        "\n",
        "Since a heart transplant is a dangerous operation and even people who successfully get heart transplants might suffer later complications, we want to look at whether a group of transplant recipients tends to survive longer than a comparison group who does not get the procedure.\n",
        "\n",
        "1. Compute (a) the proportion of people who survive in the control group who do not receive a transplant, and (b) the difference between the proportion of people who survive in the treatment group and the proportion of people who survive in the control group. In a randomized controlled trial, this is called the **average treatment effect**.\n",
        "2. Regress `y` on `transplant` using a linear model with a constant. How does the constant/intercept of the regression and the coefficient on transplant compare to your answers from part 1? Explain the relationship clearly.\n",
        "3. We'd like to include `age` in the regression, since it's reasonable to expect that older patients are less likely to survive an extensive surgery like a heart transplant. Regress `y` on a constant, transplant, and age. How does the intercept change?\n",
        "4. Build a more flexible model that allows for non-linear age effects and interactions between age and treatment. Use a train-test split to validate your model. Estimate your best model, predict the survival probability by age, and plot your results conditional on receiving a transplant and not. Describe what you see.\n",
        "5. Imagine someone suggests using these kinds of models to select who receives organ transplants; perhaps the CDC or NIH starts using a scoring algorithm to decide who is contacted about a potential organ. What are your concerns about how it is built and how it is deployed?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9b3a79a",
      "metadata": {
        "id": "d9b3a79a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}